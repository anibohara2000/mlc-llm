





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Model Prebuilts &mdash; mlc-llm 0.1.0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/tabs.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/sphinx_highlight.js"></script>
        <script src="_static/tabs.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <script type="text/javascript" src="_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Install TVM Unity Compiler" href="install/tvm.html" />
    <link rel="prev" title="Define New Model Architectures" href="tutorials/customize/define_new_models.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://llm.mlc.ai/>Home</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/mlc-ai/mlc-llm>Github</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://discord.gg/9Xpy2HGBuD>Discord Server</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   Other Resources
                 </button>
                 <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  Other Resources
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://mlc.ai/>MLC Course</a>
                     </li>
                     <li>
                       <a href=https://blog.mlc.ai/>MLC Blog</a>
                     </li>
                     <li>
                       <a href=https://webllm.mlc.ai/>Web LLM</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html">
          

          
            
            <img src="_static/mlc-logo-with-text-landscape.svg" class="logo" alt="Logo"/>
          
          </a>

          
            
            
                <div class="version">
                  0.1.0
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="get_started/project_overview.html">Project Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="get_started/mlc_chat_config.html">Configure MLCChat in JSON</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Build and Deploy Apps</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="deploy/javascript.html">WebLLM and Javascript API</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy/rest.html">Rest API</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy/cli.html">CLI and C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy/python.html">Python API and Gradio Frontend</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy/ios.html">iOS App and Swift API</a></li>
<li class="toctree-l1"><a class="reference internal" href="deploy/android.html">Android App</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Compile Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="compilation/compile_models.html">Compile Models via MLC</a></li>
<li class="toctree-l1"><a class="reference internal" href="compilation/distribute_compiled_models.html">Distribute Compiled Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="compilation/python.html">Python API for Model Compilation</a></li>
<li class="toctree-l1"><a class="reference internal" href="compilation/configure_quantization.html">🚧 Configure Quantization</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Define Model Architectures</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tutorials/customize/define_new_models.html">Define New Model Architectures</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Prebuilt Models</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Model Prebuilts</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prerequisite-model-libraries-and-compiled-weights">Prerequisite: Model Libraries and Compiled Weights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#using-prebuilt-models-for-different-platforms">Using Prebuilt Models for Different Platforms</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prebuilt-models-on-cli-python">Prebuilt Models on CLI / Python</a></li>
<li class="toctree-l3"><a class="reference internal" href="#prebuilt-models-on-ios">Prebuilt Models on iOS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#prebuilt-models-on-android">Prebuilt Models on Android</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#level-1-supported-model-architectures-the-all-in-one-table">Level 1: Supported Model Architectures (The All-In-One Table)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#level-2-model-library-tables-precompiled-binary-files">Level 2: Model Library Tables (Precompiled Binary Files)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#llama">Llama</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gpt-neox-redpajama-incite">GPT-NeoX (RedPajama-INCITE)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rwkv">RWKV</a></li>
<li class="toctree-l3"><a class="reference internal" href="#gptbigcode">GPTBigCode</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#level-3-model-variant-tables-precompiled-weights">Level 3: Model Variant Tables (Precompiled Weights)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#llama-2">Llama-2</a></li>
<li class="toctree-l3"><a class="reference internal" href="#code-llama">Code Llama</a></li>
<li class="toctree-l3"><a class="reference internal" href="#vicuna">Vicuna</a></li>
<li class="toctree-l3"><a class="reference internal" href="#wizardlm">WizardLM</a></li>
<li class="toctree-l3"><a class="reference internal" href="#wizardmath">WizardMath</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openorca-platypus2">OpenOrca Platypus2</a></li>
<li class="toctree-l3"><a class="reference internal" href="#flagalpha-llama-2-chinese">FlagAlpha Llama-2 Chinese</a></li>
<li class="toctree-l3"><a class="reference internal" href="#llama2-uncensored-georgesung">Llama2 uncensored (georgesung)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#redpajama">RedPajama</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rwkv-raven">RWKV-raven</a></li>
<li class="toctree-l3"><a class="reference internal" href="#wizardcoder">WizardCoder</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#contribute-models-to-mlc-llm">Contribute Models to MLC-LLM</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Dependency Installation</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="install/tvm.html">Install TVM Unity Compiler</a></li>
<li class="toctree-l1"><a class="reference internal" href="install/mlc_llm.html">Install MLC LLM Python Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="install/conda.html">Install Conda</a></li>
<li class="toctree-l1"><a class="reference internal" href="install/gpu.html">GPU Drivers and SDKs</a></li>
<li class="toctree-l1"><a class="reference internal" href="install/emcc.html">Install Wasm Build Environment</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Community</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="community/guideline.html">Community Guideline</a></li>
<li class="toctree-l1"><a class="reference internal" href="community/faq.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Privacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="privacy.html">MLC Chat App Privacy</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- mlc-llm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Model Prebuilts</li>
    
    
      
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/mlc-ai/mlc-llm/edit/main/docs/prebuilt_models.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="model-prebuilts">
<span id="id1"></span><h1>Model Prebuilts<a class="headerlink" href="#model-prebuilts" title="Permalink to this heading">¶</a></h1>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#overview" id="id22">Overview</a></p>
<ul>
<li><p><a class="reference internal" href="#prerequisite-model-libraries-and-compiled-weights" id="id23">Prerequisite: Model Libraries and Compiled Weights</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#using-prebuilt-models-for-different-platforms" id="id24">Using Prebuilt Models for Different Platforms</a></p>
<ul>
<li><p><a class="reference internal" href="#prebuilt-models-on-cli-python" id="id25">Prebuilt Models on CLI / Python</a></p></li>
<li><p><a class="reference internal" href="#prebuilt-models-on-ios" id="id26">Prebuilt Models on iOS</a></p></li>
<li><p><a class="reference internal" href="#prebuilt-models-on-android" id="id27">Prebuilt Models on Android</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#level-1-supported-model-architectures-the-all-in-one-table" id="id28">Level 1: Supported Model Architectures (The All-In-One Table)</a></p></li>
<li><p><a class="reference internal" href="#level-2-model-library-tables-precompiled-binary-files" id="id29">Level 2: Model Library Tables (Precompiled Binary Files)</a></p>
<ul>
<li><p><a class="reference internal" href="#llama" id="id30">Llama</a></p></li>
<li><p><a class="reference internal" href="#gpt-neox-redpajama-incite" id="id31">GPT-NeoX (RedPajama-INCITE)</a></p></li>
<li><p><a class="reference internal" href="#rwkv" id="id32">RWKV</a></p></li>
<li><p><a class="reference internal" href="#gptbigcode" id="id33">GPTBigCode</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#level-3-model-variant-tables-precompiled-weights" id="id34">Level 3: Model Variant Tables (Precompiled Weights)</a></p>
<ul>
<li><p><a class="reference internal" href="#llama-2" id="id35">Llama-2</a></p></li>
<li><p><a class="reference internal" href="#code-llama" id="id36">Code Llama</a></p></li>
<li><p><a class="reference internal" href="#vicuna" id="id37">Vicuna</a></p></li>
<li><p><a class="reference internal" href="#wizardlm" id="id38">WizardLM</a></p></li>
<li><p><a class="reference internal" href="#wizardmath" id="id39">WizardMath</a></p></li>
<li><p><a class="reference internal" href="#openorca-platypus2" id="id40">OpenOrca Platypus2</a></p></li>
<li><p><a class="reference internal" href="#flagalpha-llama-2-chinese" id="id41">FlagAlpha Llama-2 Chinese</a></p></li>
<li><p><a class="reference internal" href="#llama2-uncensored-georgesung" id="id42">Llama2 uncensored (georgesung)</a></p></li>
<li><p><a class="reference internal" href="#redpajama" id="id43">RedPajama</a></p></li>
<li><p><a class="reference internal" href="#rwkv-raven" id="id44">RWKV-raven</a></p></li>
<li><p><a class="reference internal" href="#wizardcoder" id="id45">WizardCoder</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#contribute-models-to-mlc-llm" id="id46">Contribute Models to MLC-LLM</a></p></li>
</ul>
</nav>
<section id="overview">
<span id="model-prebuilts-overview"></span><h2><a class="toc-backref" href="#id22" role="doc-backlink">Overview</a><a class="headerlink" href="#overview" title="Permalink to this heading">¶</a></h2>
<p>MLC-LLM is a universal solution for deploying different language models. Any models that can be described in <a class="reference external" href="https://mlc.ai/chapter_graph_optimization/index.html">TVM Relax</a>
(a general representation for Neural Networks and can be imported from models written in PyTorch) can be recognized by MLC-LLM and thus deployed to different backends with the
help of <a class="reference internal" href="install/tvm.html"><span class="doc">TVM Unity</span></a>.</p>
<p>There are two ways to run a model on MLC-LLM:</p>
<ol class="arabic simple">
<li><p>Compile your own models following <a class="reference internal" href="compilation/compile_models.html"><span class="doc">the model compilation page</span></a>.</p></li>
<li><p>Use off-the-shelf prebuilts models following this current page.</p></li>
</ol>
<p>This page focuses on the second option:</p>
<ul class="simple">
<li><p>Documenting <a class="reference internal" href="#using-model-prebuilts"><span class="std std-ref">how to use prebuilts</span></a> for various platforms, and</p></li>
<li><p>Tracking what current <a class="reference internal" href="#supported-model-architectures"><span class="std std-ref">prebuilt models we provide</span></a>.</p></li>
</ul>
<section id="prerequisite-model-libraries-and-compiled-weights">
<h3><a class="toc-backref" href="#id23" role="doc-backlink">Prerequisite: Model Libraries and Compiled Weights</a><a class="headerlink" href="#prerequisite-model-libraries-and-compiled-weights" title="Permalink to this heading">¶</a></h3>
<p>In order to run a specific model on MLC-LLM, you need:</p>
<p><strong>1. A model library:</strong> a binary file containing the end-to-end functionality to inference a model (e.g. <code class="docutils literal notranslate"><span class="pre">Llama-2-7b-chat-hf-q4f16_1-cuda.so</span></code>). See the full list of all precompiled model libraries <a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs">here</a>.</p>
<p><strong>2. Compiled weights:</strong> a folder containing multiple files that store the compiled and quantized weights of a model (e.g. <a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1">https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1</a>).  See the full list of all precompiled weights <a class="reference external" href="https://huggingface.co/mlc-ai">here</a>.</p>
</section>
</section>
<section id="using-prebuilt-models-for-different-platforms">
<span id="using-model-prebuilts"></span><h2><a class="toc-backref" href="#id24" role="doc-backlink">Using Prebuilt Models for Different Platforms</a><a class="headerlink" href="#using-prebuilt-models-for-different-platforms" title="Permalink to this heading">¶</a></h2>
<p>We quickly go over how to use prebuilt models for each platform. You can find detailed instruction on each platform’s corresponding page.</p>
<section id="prebuilt-models-on-cli-python">
<span id="using-prebuilt-models-cli"></span><h3><a class="toc-backref" href="#id25" role="doc-backlink">Prebuilt Models on CLI / Python</a><a class="headerlink" href="#prebuilt-models-on-cli-python" title="Permalink to this heading">¶</a></h3>
<p>For more, please see <a class="reference internal" href="deploy/cli.html"><span class="doc">the CLI page</span></a>, and the <a class="reference internal" href="deploy/python.html"><span class="doc">the Python page</span></a>.</p>
<details class="summary-click-to-show-details">
<summary>Click to show details</summary><p>First create the conda environment if you have not done so.</p>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>mlc-chat-venv<span class="w"> </span>-c<span class="w"> </span>mlc-ai<span class="w"> </span>-c<span class="w"> </span>conda-forge<span class="w"> </span>mlc-chat-cli-nightly
conda<span class="w"> </span>activate<span class="w"> </span>mlc-chat-venv
conda<span class="w"> </span>install<span class="w"> </span>git<span class="w"> </span>git-lfs
git<span class="w"> </span>lfs<span class="w"> </span>install
</pre></div>
</div>
</div></blockquote>
<p>Download the prebuilt model libraries from github.</p>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>mkdir<span class="w"> </span>-p<span class="w"> </span>dist/prebuilt
git<span class="w"> </span>clone<span class="w"> </span>https://github.com/mlc-ai/binary-mlc-llm-libs.git<span class="w"> </span>dist/prebuilt/lib
</pre></div>
</div>
</div></blockquote>
<p>Download the prebuilt model weights from hugging face for the model variant you want.</p>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># Say we want to run rwkv-raven-7b-q8f16_0</span>
<span class="nb">cd</span><span class="w"> </span>dist/prebuilt
git<span class="w"> </span>clone<span class="w"> </span>https://huggingface.co/mlc-ai/mlc-chat-rwkv-raven-7b-q8f16_0
<span class="nb">cd</span><span class="w"> </span>../..

<span class="c1"># The format being:</span>
<span class="c1"># cd dist/prebuilt</span>
<span class="c1"># git clone https://huggingface.co/mlc-ai/mlc-chat-[model-code]</span>
<span class="c1"># cd ../..</span>
<span class="c1"># mlc_chat_cli --model [model-code]</span>
</pre></div>
</div>
</div></blockquote>
<p>Run the model with CLI:</p>
<blockquote>
<div><div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="c1"># For CLI</span>
mlc_chat_cli<span class="w"> </span>--model<span class="w"> </span>rwkv-raven-7b-q8f16_0
</pre></div>
</div>
</div></blockquote>
<p>To run the model with Python API, see <a class="reference internal" href="deploy/python.html"><span class="doc">the Python page</span></a> (all other downloading steps are the same as CLI).</p>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
</section>
<section id="prebuilt-models-on-ios">
<span id="using-prebuilt-models-ios"></span><h3><a class="toc-backref" href="#id26" role="doc-backlink">Prebuilt Models on iOS</a><a class="headerlink" href="#prebuilt-models-on-ios" title="Permalink to this heading">¶</a></h3>
<p>For more, please see <a class="reference internal" href="deploy/ios.html"><span class="doc">the iOS page</span></a>.</p>
<details class="summary-click-to-show-details">
<summary>Click to show details</summary><p>The <a class="reference external" href="https://apps.apple.com/us/app/mlc-chat/id6448482937">iOS app</a> has builtin RedPajama-3B and Llama-2-7b support.</p>
<p>All prebuilt models with an entry in <code class="docutils literal notranslate"><span class="pre">iOS</span></code> in the <a class="reference internal" href="#model-library-tables"><span class="std std-ref">model library table</span></a> are supported by iOS. Namely, we have:</p>
<table class="docutils align-default" id="id3">
<caption><span class="caption-text">Prebuilt model libraries integrated in the iOS app</span><a class="headerlink" href="#id3" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 33%" />
<col style="width: 33%" />
<col style="width: 33%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model library name</p></th>
<th class="head"><p>Model Family</p></th>
<th class="head"><p>Quantization Mode</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><cite>Llama-2-7b-chat-hf-q3f16_1</cite></p></td>
<td><p>LLaMA</p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int3</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><cite>vicuna-v1-7b-q3f16_0</cite></p></td>
<td><p>LLaMA</p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int3</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><cite>RedPajama-INCITE-Chat-3B-v1-q4f16_1</cite></p></td>
<td><p>GPT-NeoX</p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int4</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>As for prebuilt model weights, the ones we have integrated into app are listed below:</p>
<table class="docutils align-default" id="id4">
<caption><span class="caption-text">Tested prebuilt model weights for iOS</span><a class="headerlink" href="#id4" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model code</p></th>
<th class="head"><p>Model Series</p></th>
<th class="head"><p>Quantization Mode</p></th>
<th class="head"><p>Hugging Face repo</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><cite>Llama-2-7b-q3f16_1</cite></p></td>
<td><p><a class="reference external" href="https://ai.meta.com/llama/">Llama</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int3</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q3f16_1">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><cite>vicuna-v1-7b-q3f16_0</cite></p></td>
<td><p><a class="reference external" href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int3</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-vicuna-v1-7b-q3f16_0">link</a></p></td>
</tr>
<tr class="row-even"><td><p><cite>RedPajama-INCITE-Chat-3B-v1-q4f16_1</cite></p></td>
<td><p><a class="reference external" href="https://www.together.xyz/blog/redpajama">RedPajama</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int4</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f16_1">link</a></p></td>
</tr>
</tbody>
</table>
<p>To run a model variant you compiled on your own, you can directly reuse the above integrated prebuilt model libraries, as long as the model shares the architecture and is compiled with the same quantization mode. For example, if you compile <a class="reference external" href="https://github.com/openlm-research/open_llama">OpenLLaMA-7B</a> with quantization mode <code class="docutils literal notranslate"><span class="pre">q3f16_0</span></code>, then you can run the compiled OpenLLaMA model on iPhone without rebuilding the iOS app by reusing the <cite>vicuna-v1-7b-q3f16_0</cite> model library. Then you can upload the compiled weights to hugging face so that you can download the weights in the app as shown below (for more on uploading to hugging face, please check the <a class="reference internal" href="compilation/distribute_compiled_models.html"><span class="doc">model distribution page</span></a>).</p>
<p>To add a model to the iOS app, follow the steps below:</p>
<div class="sphinx-tabs docutils container">
<div aria-label="Tabbed content" class="closeable" role="tablist"><button aria-controls="panel-0-0-0" aria-selected="true" class="sphinx-tabs-tab" id="tab-0-0-0" name="0-0" role="tab" tabindex="0">Step 1</button><button aria-controls="panel-0-0-1" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-1" name="0-1" role="tab" tabindex="-1">Step 2</button><button aria-controls="panel-0-0-2" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-2" name="0-2" role="tab" tabindex="-1">Step 3</button><button aria-controls="panel-0-0-3" aria-selected="false" class="sphinx-tabs-tab" id="tab-0-0-3" name="0-3" role="tab" tabindex="-1">Step 4</button></div><div aria-labelledby="tab-0-0-0" class="sphinx-tabs-panel" id="panel-0-0-0" name="0-0" role="tabpanel" tabindex="0"><p>Open “MLCChat” app, click “Add model variant”.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-1.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-1.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-1.png" style="width: 30%;" /></a>
</div><div aria-labelledby="tab-0-0-1" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-1" name="0-1" role="tabpanel" tabindex="0"><p>Paste the repository URL of the model built on your own, and click “Add”.</p>
<p>You can refer to the link in the image as an example.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-2.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-2.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-2.png" style="width: 30%;" /></a>
</div><div aria-labelledby="tab-0-0-2" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-2" name="0-2" role="tabpanel" tabindex="0"><p>After adding the model, you can download your model from the URL by clicking the download button.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-3.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-3.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-3.png" style="width: 30%;" /></a>
</div><div aria-labelledby="tab-0-0-3" class="sphinx-tabs-panel" hidden="true" id="panel-0-0-3" name="0-3" role="tabpanel" tabindex="0"><p>When the download is finished, click into the model and enjoy.</p>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-4.png"><img alt="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-4.png" class="align-center" src="https://raw.githubusercontent.com/mlc-ai/web-data/main/images/mlc-llm/tutorials/iPhone-custom-4.png" style="width: 30%;" /></a>
</div></div>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
</section>
<section id="prebuilt-models-on-android">
<span id="prebuilt-models-android"></span><h3><a class="toc-backref" href="#id27" role="doc-backlink">Prebuilt Models on Android</a><a class="headerlink" href="#prebuilt-models-on-android" title="Permalink to this heading">¶</a></h3>
<p>For more, please see <a class="reference internal" href="deploy/android.html"><span class="doc">the Android page</span></a>.</p>
<details class="summary-click-to-show-details">
<summary>Click to show details</summary><p>The apk for demo Android app includes the following models. To add more, check out the Android page.</p>
<table class="docutils align-default" id="id5">
<caption><span class="caption-text">Prebuilt Models for Android</span><a class="headerlink" href="#id5" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model code</p></th>
<th class="head"><p>Model Series</p></th>
<th class="head"><p>Quantization Mode</p></th>
<th class="head"><p>Hugging Face repo</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><cite>Llama-2-7b-q4f16_1</cite></p></td>
<td><p><a class="reference external" href="https://ai.meta.com/llama/">Llama</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int4</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1">link</a></p></td>
</tr>
<tr class="row-odd"><td><p><cite>RedPajama-INCITE-Chat-3B-v1-q4f16_1</cite></p></td>
<td><p><a class="reference external" href="https://www.together.xyz/blog/redpajama">RedPajama</a></p></td>
<td><ul class="simple">
<li><p>Weight storage data type: int4</p></li>
<li><p>Running data type: float16</p></li>
<li><p>Symmetric quantization</p></li>
</ul>
</td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f16_1">link</a></p></td>
</tr>
</tbody>
</table>
</details><div class="line-block">
<div class="line"><br /></div>
</div>
</section>
</section>
<section id="level-1-supported-model-architectures-the-all-in-one-table">
<span id="supported-model-architectures"></span><h2><a class="toc-backref" href="#id28" role="doc-backlink">Level 1: Supported Model Architectures (The All-In-One Table)</a><a class="headerlink" href="#level-1-supported-model-architectures-the-all-in-one-table" title="Permalink to this heading">¶</a></h2>
<p>For each model architecture (e.g. Llama), there are multiple variants (e.g. CodeLlama, WizardLM). The variants share the same code for inference and only differ in their weights. In other words, running CodeLlama and WizardLM can use the same model library file (specified in Level 2 tables), but different precompiled weights (specified in Level 3 tables). Note that we have not provided prebuilt weights for all model variants.</p>
<p>Each entry below hyperlinks to the corresponding level 2 and level 3 tables.</p>
<p>MLC-LLM supports the following model architectures:</p>
<table class="docutils align-default" id="id6">
<caption><span class="caption-text">Supported Model Architectures</span><a class="headerlink" href="#id6" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 20%" />
<col style="width: 20%" />
<col style="width: 30%" />
<col style="width: 30%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Model Architecture</p></th>
<th class="head"><p>Support</p></th>
<th class="head"><p>Available MLC Prebuilts</p></th>
<th class="head"><p>Unavailable in MLC Prebuilts</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/facebookresearch/llama">LLaMA</a></p></td>
<td><ul class="simple">
<li><p><a class="reference internal" href="#llama-library-table"><span class="std std-ref">Prebuilt Model Library</span></a></p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/relax_model/llama.py">MLC Implementation</a></p></li>
</ul>
</td>
<td><ul class="simple">
<li><p><a class="reference internal" href="#llama2-variant-table"><span class="std std-ref">Llama-2</span></a></p></li>
<li><p><a class="reference internal" href="#code-llama-variant-table"><span class="std std-ref">Code Llama</span></a></p></li>
<li><p><a class="reference internal" href="#vicuna-variant-table"><span class="std std-ref">Vicuna</span></a></p></li>
<li><p><a class="reference internal" href="#wizardlm-variant-table"><span class="std std-ref">WizardLM</span></a></p></li>
<li><p><a class="reference internal" href="#wizard-math-variant-table"><span class="std std-ref">WizardMath</span></a></p></li>
<li><p><a class="reference internal" href="#open-orca-variant-table"><span class="std std-ref">OpenOrca Platypus2</span></a></p></li>
<li><p><a class="reference internal" href="#flag-alpha-llama2-variant-table"><span class="std std-ref">FlagAlpha Llama-2 Chinese</span></a></p></li>
<li><p><a class="reference internal" href="#llama2-uncensored-variant-table"><span class="std std-ref">georgesung Llama-2 Uncensored</span></a></p></li>
</ul>
</td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://github.com/tatsu-lab/stanford_alpaca">Alpaca</a></p></li>
<li><p><a class="reference external" href="https://github.com/artidoro/qlora">Guanaco</a></p></li>
<li><p><a class="reference external" href="https://github.com/openlm-research/open_llama">OpenLLaMA</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/gorilla-llm/gorilla-7b-hf-delta-v0">Gorilla</a></p></li>
<li><p><a class="reference external" href="https://github.com/RUC-GSAI/YuLan-Chat">YuLan-Chat</a></p></li>
<li><p><a class="reference external" href="https://github.com/nlpxucan/WizardLM/tree/main/WizardCoder">WizardCoder (new)</a></p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/EleutherAI/gpt-neox">GPT-NeoX</a></p></td>
<td><ul class="simple">
<li><p><a class="reference internal" href="#gpt-neox-library-table"><span class="std std-ref">Prebuilt Model Library</span></a></p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/relax_model/gpt_neox.py">MLC Implementation</a></p></li>
</ul>
</td>
<td><ul class="simple">
<li><p><a class="reference internal" href="#red-pajama-variant-table"><span class="std std-ref">RedPajama</span></a></p></li>
</ul>
</td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://github.com/databrickslabs/dolly">Dolly</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/EleutherAI/pythia-1.4b">Pythia</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/stabilityai/stablecode-instruct-alpha-3b">StableCode</a></p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://huggingface.co/EleutherAI/gpt-j-6b">GPT-J</a></p></td>
<td><ul class="simple">
<li><p>Prebuilt not compiled yet</p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/relax_model/gptj.py">MLC Implementation</a></p></li>
</ul>
</td>
<td></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://github.com/OpenLMLab/MOSS">MOSS</a></p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://github.com/BlinkDL/RWKV-LM">RWKV</a></p></td>
<td><ul class="simple">
<li><p><a class="reference internal" href="#rwkv-library-table"><span class="std std-ref">Prebuilt Model Library</span></a></p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/relax_model/rwkv.py">MLC Implementation</a></p></li>
</ul>
</td>
<td><ul class="simple">
<li><p><a class="reference internal" href="#rwkv-raven-variant-table"><span class="std std-ref">RWKV-raven</span></a></p></li>
</ul>
</td>
<td></td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://huggingface.co/Vision-CAIR/MiniGPT-4">MiniGPT</a></p></td>
<td><ul class="simple">
<li><p>Prebuilt not compiled yet</p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/relax_model/minigpt.py">MLC Implementation</a></p></li>
</ul>
</td>
<td></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/Vision-CAIR/MiniGPT-4">MiniGPT-4</a></p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/gpt_bigcode">GPTBigCode</a></p></td>
<td><ul class="simple">
<li><p><a class="reference internal" href="#gpt-big-code-library-table"><span class="std std-ref">Prebuilt Model Library</span></a></p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/relax_model/gpt_bigcode.py">MLC Implementation</a></p></li>
</ul>
</td>
<td><ul class="simple">
<li><p><a class="reference internal" href="#wizard-coder-variant-table"><span class="std std-ref">WizardCoder (old)</span></a></p></li>
</ul>
</td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/bigcode/starcoder">StarCoder</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/bigcode/gpt_bigcode-santacoder">SantaCoder</a></p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p><a class="reference external" href="https://github.com/THUDM/ChatGLM-6B/blob/main/README_en.md">ChatGLM</a></p></td>
<td><ul class="simple">
<li><p>Prebuilt not compiled yet</p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/relax_model/chatglm.py">MLC Implementation</a></p></li>
</ul>
</td>
<td></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/THUDM/chatglm2-6b">ChatGLM2</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/THUDM/codegeex2-6b">CodeGeeX2</a></p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p><a class="reference external" href="https://huggingface.co/stabilityai">StableLM</a></p></td>
<td><ul class="simple">
<li><p>Prebuilt not compiled yet</p></li>
<li><p><a class="reference external" href="https://github.com/mlc-ai/mlc-llm/blob/main/mlc_llm/relax_model/stablelm_3b.py">MLC Implementation</a></p></li>
</ul>
</td>
<td></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/collections/stabilityai/stable-lm-650852cfd55dd4e15cdcb30a">StableLM</a></p></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p>If the model variant you are interested in uses one of these model architectures we support (but we have not provided the prebuilt weights yet), you can check out <a class="reference internal" href="compilation/compile_models.html"><span class="doc">Compile Models via MLC</span></a> on how to compile your own models. Afterwards, you may follow <a class="reference internal" href="compilation/distribute_compiled_models.html"><span class="doc">Distribute Compiled Models</span></a> to upload your prebuilt weights to hugging face, and submit a PR that adds an entry to this page, contributing to the community.</p>
<p>For models structured in an architecture we have not supported yet, you could:</p>
<ul class="simple">
<li><p>Either <a class="reference external" href="https://github.com/mlc-ai/mlc-llm/issues/new?assignees=&amp;labels=new-models&amp;projects=&amp;template=model-request.md&amp;title=%5BModel+Request%5D+">create a [Model Request] issue</a> which automatically shows up on our <a class="reference external" href="https://github.com/orgs/mlc-ai/projects/2">Model Request Tracking Board</a>.</p></li>
<li><p>Or follow our tutorial <a class="reference internal" href="tutorials/customize/define_new_models.html"><span class="doc">Define New Models</span></a>, which introduces how to bring a new model architecture to MLC-LLM.</p></li>
</ul>
</section>
<section id="level-2-model-library-tables-precompiled-binary-files">
<span id="model-library-tables"></span><h2><a class="toc-backref" href="#id29" role="doc-backlink">Level 2: Model Library Tables (Precompiled Binary Files)</a><a class="headerlink" href="#level-2-model-library-tables-precompiled-binary-files" title="Permalink to this heading">¶</a></h2>
<p>As mentioned earlier, each model architecture corresponds to a different model library file. That is, you cannot use the same model library file to run <code class="docutils literal notranslate"><span class="pre">RedPajama</span></code> and <code class="docutils literal notranslate"><span class="pre">Llama-2</span></code>. However, you can use the same <code class="docutils literal notranslate"><span class="pre">Llama</span></code> model library file to run <code class="docutils literal notranslate"><span class="pre">Llama-2</span></code>, <code class="docutils literal notranslate"><span class="pre">WizardLM</span></code>, <code class="docutils literal notranslate"><span class="pre">CodeLlama</span></code>, etc, but just with different weight files (from tables in Level 3).</p>
<p>Each table below demonstrates the pre-compiled model library files for each model architecture. This is categorized by:</p>
<ul class="simple">
<li><p><strong>Size</strong>: each size of model has its own distinct model library file (e.g. 7B or 13B number of parameters)</p></li>
<li><p><strong>Platform</strong>: the backend that the model library is intended to be run on (e.g. CUDA, ROCm, iphone, etc.)</p></li>
<li><p><strong>Quantization scheme</strong>: the model library file also differs due to the quantization scheme used. For more on this, please see the <a class="reference internal" href="compilation/compile_models.html"><span class="doc">model compilation page</span></a> (e.g. <code class="docutils literal notranslate"><span class="pre">q3f16_1</span></code> vs. <code class="docutils literal notranslate"><span class="pre">q4f16_1</span></code>)</p></li>
</ul>
<p>Each entry links to the specific model library file found in <a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs">this github repo</a>.</p>
<section id="llama">
<span id="llama-library-table"></span><h3><a class="toc-backref" href="#id30" role="doc-backlink">Llama</a><a class="headerlink" href="#llama" title="Permalink to this heading">¶</a></h3>
<table class="docutils align-default" id="id7">
<caption><span class="caption-text">Llama</span><a class="headerlink" href="#id7" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head stub"></th>
<th class="head"><p>CUDA</p></th>
<th class="head"><p>ROCm</p></th>
<th class="head"><p>Vulkan</p>
<p>(Linux)</p>
</th>
<th class="head"><p>Vulkan</p>
<p>(Windows)</p>
</th>
<th class="head"><p>Metal</p>
<p>(M1/M2)</p>
</th>
<th class="head"><p>Metal</p>
<p>(Intel)</p>
</th>
<th class="head"><p>iOS</p></th>
<th class="head"><p>webgpu</p></th>
<th class="head"><p>mali</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><th class="stub"><p>7B</p></th>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-7b-chat-hf-q4f16_1-cuda.so">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-7b-chat-hf-q4f16_1-rocm.so">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-7b-chat-hf-q4f16_1-vulkan.so">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-7b-chat-hf-q4f16_1-vulkan.dll">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-7b-chat-hf-q4f16_1-metal.so">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-7b-chat-hf-q4f16_1-metal_x86_64.dylib">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-7b-chat-hf-q3f16_1-iphone.tar">q3f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-7b-chat-hf-q4f16_1-webgpu.wasm">q4f16_1</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-7b-chat-hf-q4f32_1-webgpu.wasm">q4f32_1</a></p>
</td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-7b-chat-hf-q4f16_1-mali.so">q4f16_1</a></p></td>
</tr>
<tr class="row-odd"><th class="stub"><p>13B</p></th>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-13b-chat-hf-q4f16_1-cuda.so">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-13b-chat-hf-q4f16_1-rocm.so">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-13b-chat-hf-q4f16_1-vulkan.so">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-13b-chat-hf-q4f16_1-vulkan.dll">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-13b-chat-hf-q4f16_1-metal.so">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-13b-chat-hf-q4f16_1-metal_x86_64.dylib">q4f16_1</a></p></td>
<td></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-13b-chat-hf-q4f16_1-webgpu.wasm">q4f16_1</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-13b-chat-hf-q4f32_1-webgpu.wasm">q4f32_1</a></p>
</td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-13b-chat-hf-q4f16_1-mali.so">q4f16_1</a></p></td>
</tr>
<tr class="row-even"><th class="stub"><p>34B</p></th>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/CodeLlama-34b-hf-q4f16_1-cuda.so">q4f16_1</a></p></td>
<td></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/CodeLlama-34b-hf-q4f16_1-vulkan.so">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/CodeLlama-34b-hf-q4f16_1-vulkan.dll">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/CodeLlama-34b-hf-q4f16_1-metal.so">q4f16_1</a></p></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p>70B</p></th>
<td></td>
<td></td>
<td></td>
<td></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-70b-chat-hf-q3f16_1-metal.so">q3f16_1</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-70b-chat-hf-q4f16_1-metal.so">q4f16_1</a></p>
</td>
<td></td>
<td></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/Llama-2-70b-chat-hf-q4f16_1-webgpu.wasm">q4f16_1</a></p></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="gpt-neox-redpajama-incite">
<span id="gpt-neox-library-table"></span><h3><a class="toc-backref" href="#id31" role="doc-backlink">GPT-NeoX (RedPajama-INCITE)</a><a class="headerlink" href="#gpt-neox-redpajama-incite" title="Permalink to this heading">¶</a></h3>
<table class="docutils align-default" id="id8">
<caption><span class="caption-text">GPT-NeoX (RedPajama-INCITE)</span><a class="headerlink" href="#id8" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head stub"></th>
<th class="head"><p>CUDA</p></th>
<th class="head"><p>ROCm</p></th>
<th class="head"><p>Vulkan</p>
<p>(Linux)</p>
</th>
<th class="head"><p>Vulkan</p>
<p>(Windows)</p>
</th>
<th class="head"><p>Metal</p>
<p>(M1/M2)</p>
</th>
<th class="head"><p>Metal</p>
<p>(Intel)</p>
</th>
<th class="head"><p>iOS</p></th>
<th class="head"><p>webgpu</p></th>
<th class="head"><p>mali</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><th class="stub"><p>3B</p></th>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_1-cuda.so">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_1-rocm.so">q4f16_1</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_0-vulkan.so">q4f16_0</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_1-vulkan.so">q4f16_1</a></p>
</td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_0-vulkan.dll">q4f16_0</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_1-vulkan.dll">q4f16_1</a></p>
</td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_0-metal.so">q4f16_0</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_1-metal.so">q4f16_1</a></p>
</td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_0-metal_x86_64.dylib">q4f16_0</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_1-metal_x86_64.dylib">q4f16_1</a></p>
</td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_0-iphone.tar">q4f16_0</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_1-iphone.tar">q4f16_1</a></p>
</td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_0-webgpu-v1.wasm">q4f16_0</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_1-webgpu.wasm">q4f16_1</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f32_0-webgpu-v1.wasm">q4f32_0</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f32_1-webgpu.wasm">q4f32_1</a></p>
</td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/RedPajama-INCITE-Chat-3B-v1-q4f16_1-mali.so">q4f16_1</a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="rwkv">
<span id="rwkv-library-table"></span><h3><a class="toc-backref" href="#id32" role="doc-backlink">RWKV</a><a class="headerlink" href="#rwkv" title="Permalink to this heading">¶</a></h3>
<table class="docutils align-default" id="id9">
<caption><span class="caption-text">RWKV</span><a class="headerlink" href="#id9" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head stub"></th>
<th class="head"><p>CUDA</p></th>
<th class="head"><p>ROCm</p></th>
<th class="head"><p>Vulkan</p>
<p>(Linux)</p>
</th>
<th class="head"><p>Vulkan</p>
<p>(Windows)</p>
</th>
<th class="head"><p>Metal</p>
<p>(M1/M2)</p>
</th>
<th class="head"><p>Metal</p>
<p>(Intel)</p>
</th>
<th class="head"><p>iOS</p></th>
<th class="head"><p>webgpu</p></th>
<th class="head"><p>mali</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><th class="stub"><p>1B5</p></th>
<td></td>
<td></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/rwkv-raven-1b5-q8f16_0-vulkan.so">q8f16_0</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/rwkv-raven-1b5-q8f16_0-vulkan.dll">q8f16_0</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/rwkv-raven-1b5-q8f16_0-metal.so">q8f16_0</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/rwkv-raven-1b5-q8f16_0-metal_x86_64.dylib">q8f16_0</a></p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-odd"><th class="stub"><p>3B</p></th>
<td></td>
<td></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/rwkv-raven-3b-q8f16_0-vulkan.so">q8f16_0</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/rwkv-raven-3b-q8f16_0-vulkan.dll">q8f16_0</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/rwkv-raven-3b-q8f16_0-metal.so">q8f16_0</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/rwkv-raven-3b-q8f16_0-metal_x86_64.dylib">q8f16_0</a></p></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><th class="stub"><p>7B</p></th>
<td></td>
<td></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/rwkv-raven-7b-q8f16_0-vulkan.so">q8f16_0</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/rwkv-raven-7b-q8f16_0-vulkan.dll">q8f16_0</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/rwkv-raven-7b-q8f16_0-metal.so">q8f16_0</a></p></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/rwkv-raven-7b-q8f16_0-metal_x86_64.dylib">q8f16_0</a></p></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="gptbigcode">
<span id="gpt-big-code-library-table"></span><h3><a class="toc-backref" href="#id33" role="doc-backlink">GPTBigCode</a><a class="headerlink" href="#gptbigcode" title="Permalink to this heading">¶</a></h3>
<p>Note that these all links to model libraries for WizardCoder (the older version released in Jun. 2023).
However, any GPTBigCode model variants should be able to reuse these (e.g. StarCoder, SantaCoder).</p>
<table class="docutils align-default" id="id10">
<caption><span class="caption-text">GPTBigCode</span><a class="headerlink" href="#id10" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head stub"></th>
<th class="head"><p>CUDA</p></th>
<th class="head"><p>ROCm</p></th>
<th class="head"><p>Vulkan</p>
<p>(Linux)</p>
</th>
<th class="head"><p>Vulkan</p>
<p>(Windows)</p>
</th>
<th class="head"><p>Metal</p>
<p>(M1/M2)</p>
</th>
<th class="head"><p>Metal</p>
<p>(Intel)</p>
</th>
<th class="head"><p>iOS</p></th>
<th class="head"><p>webgpu</p></th>
<th class="head"><p>mali</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><th class="stub"><p>15B</p></th>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/WizardCoder-15B-V1.0-q4f16_1-cuda.so">q4f16_1</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/WizardCoder-15B-V1.0-q4f32_1-cuda.so">q4f32_1</a></p>
</td>
<td></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/WizardCoder-15B-V1.0-q4f16_1-vulkan.so">q4f16_1</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/WizardCoder-15B-V1.0-q4f32_1-vulkan.so">q4f32_1</a></p>
</td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/WizardCoder-15B-V1.0-q4f16_1-vulkan.dll">q4f16_1</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/WizardCoder-15B-V1.0-q4f32_1-vulkan.dll">q4f32_1</a></p>
</td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/WizardCoder-15B-V1.0-q4f16_1-metal.so">q4f16_1</a></p></td>
<td></td>
<td></td>
<td><p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/WizardCoder-15B-V1.0-q4f16_1-webgpu.wasm">q4f16_1</a></p>
<p><a class="reference external" href="https://github.com/mlc-ai/binary-mlc-llm-libs/blob/main/WizardCoder-15B-V1.0-q4f32_1-webgpu.wasm">q4f32_1</a></p>
</td>
<td></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="level-3-model-variant-tables-precompiled-weights">
<span id="model-variant-tables"></span><h2><a class="toc-backref" href="#id34" role="doc-backlink">Level 3: Model Variant Tables (Precompiled Weights)</a><a class="headerlink" href="#level-3-model-variant-tables-precompiled-weights" title="Permalink to this heading">¶</a></h2>
<p>Finally, for each model variant, we provide the precompiled weights we uploaded to hugging face.</p>
<p>Each precompiled weight is categorized by its model size (e.g. 7B vs. 13B) and the quantization scheme (e.g. <code class="docutils literal notranslate"><span class="pre">q3f16_1</span></code> vs. <code class="docutils literal notranslate"><span class="pre">q4f16_1</span></code>). We note that the weights are <strong>platform-agnostic</strong>.</p>
<p>Each model variant also loads its conversation configuration from a pre-defined <a class="reference internal" href="get_started/mlc_chat_config.html#load-predefined-conv-template"><span class="std std-ref">conversation template</span></a>. Note that multiple model variants can share a common conversation template.</p>
<p>Some of these files are uploaded by our community contributors–thank you!</p>
<section id="llama-2">
<span id="llama2-variant-table"></span><h3><a class="reference external" href="https://ai.meta.com/llama/">Llama-2</a><a class="headerlink" href="#llama-2" title="Permalink to this heading">¶</a></h3>
<p>Conversation template: <code class="docutils literal notranslate"><span class="pre">llama-2</span></code></p>
<table class="docutils align-default" id="id11">
<caption><span class="caption-text">Llama-2</span><a class="headerlink" href="#id11" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Size</p></th>
<th class="head"><p>Hugging Face Repo Link</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>7B</p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q3f16_1">q3f16_1</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f16_1">q4f16_1</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-Llama-2-7b-chat-hf-q4f32_1">q4f32_1</a></p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>13B</p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-Llama-2-13b-chat-hf-q4f16_1">q4f16_1</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-Llama-2-13b-chat-hf-q4f32_1">q4f32_1</a></p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>70B</p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-Llama-2-70b-chat-hf-q3f16_1">q3f16_1</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-Llama-2-70b-chat-hf-q4f16_1">q4f16_1</a></p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</section>
<section id="code-llama">
<span id="code-llama-variant-table"></span><h3><a class="reference external" href="https://about.fb.com/news/2023/08/code-llama-ai-for-coding/">Code Llama</a><a class="headerlink" href="#code-llama" title="Permalink to this heading">¶</a></h3>
<p>Conversation template: <code class="docutils literal notranslate"><span class="pre">codellama_completion</span></code></p>
<table class="docutils align-default" id="id12">
<caption><span class="caption-text">Code Llama</span><a class="headerlink" href="#id12" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Size</p></th>
<th class="head"><p>Hugging Face Repo Link</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>7B</p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-CodeLlama-7b-hf-q4f16_1">q4f16_1 (Base)</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-CodeLlama-7b-Instruct-hf-q4f16_1">q4f16_1 (Instruct)</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-CodeLlama-7b-Python-hf-q4f16_1">q4f16_1 (Python)</a></p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>13B</p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-CodeLlama-13b-hf-q4f16_1">q4f16_1 (Base)</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-CodeLlama-13b-Instruct-hf-q4f16_1">q4f16_1 (Instruct)</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-CodeLlama-13b-Python-hf-q4f16_1">q4f16_1 (Python)</a></p></li>
</ul>
</td>
</tr>
<tr class="row-even"><td><p>34B</p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-CodeLlama-34b-hf-q4f16_1">q4f16_1 (Base)</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-CodeLlama-34b-Instruct-hf-q4f16_1">q4f16_1 (Instruct)</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-CodeLlama-34b-Python-hf-q4f16_1">q4f16_1 (Python)</a></p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</section>
<section id="vicuna">
<span id="vicuna-variant-table"></span><h3><a class="reference external" href="https://lmsys.org/blog/2023-03-30-vicuna/">Vicuna</a><a class="headerlink" href="#vicuna" title="Permalink to this heading">¶</a></h3>
<p>Conversation template: <code class="docutils literal notranslate"><span class="pre">vicuna_v1.1</span></code></p>
<table class="docutils align-default" id="id13">
<caption><span class="caption-text">Vicuna</span><a class="headerlink" href="#id13" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Size</p></th>
<th class="head"><p>Hugging Face Repo Link</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>7B</p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-vicuna-v1-7b-q3f16_0">q3f16_0</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-vicuna-v1-7b-q4f32_0">q4f32_0</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/demo-vicuna-v1-7b-int3">int3 (demo)</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/demo-vicuna-v1-7b-int4">int4 (demo)</a></p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</section>
<section id="wizardlm">
<span id="wizardlm-variant-table"></span><h3><a class="reference external" href="https://github.com/nlpxucan/WizardLM">WizardLM</a><a class="headerlink" href="#wizardlm" title="Permalink to this heading">¶</a></h3>
<p>Conversation template: <code class="docutils literal notranslate"><span class="pre">vicuna_v1.1</span></code></p>
<table class="docutils align-default" id="id14">
<caption><span class="caption-text">WizardLM</span><a class="headerlink" href="#id14" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Size</p></th>
<th class="head"><p>Hugging Face Repo Link</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>13B</p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-WizardLM-13B-V1.2-q4f16_1">q4f16_1 (V1.2)</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-WizardLM-13B-V1.2-q4f32_1">q4f32_1 (V1.2)</a></p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>70B</p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-WizardLM-70B-V1.0-q3f16_1">q3f16_1 (V1.0)</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-WizardLM-70B-V1.0-q4f16_1">q4f16_1 (V1.0)</a></p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</section>
<section id="wizardmath">
<span id="wizard-math-variant-table"></span><h3><a class="reference external" href="https://github.com/nlpxucan/WizardLM/tree/main/WizardMath">WizardMath</a><a class="headerlink" href="#wizardmath" title="Permalink to this heading">¶</a></h3>
<p>Conversation template: <code class="docutils literal notranslate"><span class="pre">wizard_coder_or_math</span></code></p>
<table class="docutils align-default" id="id15">
<caption><span class="caption-text">WizardMath</span><a class="headerlink" href="#id15" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Size</p></th>
<th class="head"><p>Hugging Face Repo Link</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>7B</p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-WizardMath-7B-V1.0-q4f16_1">q4f16_1</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-WizardMath-7B-V1.0-q4f32_1">q4f32_1</a></p></li>
</ul>
</td>
</tr>
<tr class="row-odd"><td><p>13B</p></td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-WizardMath-13B-V1.0-q4f16_1">q4f16_1</a></p></td>
</tr>
<tr class="row-even"><td><p>70B</p></td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-WizardMath-70B-V1.0-q4f16_1">q4f16_1</a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="openorca-platypus2">
<span id="open-orca-variant-table"></span><h3><a class="reference external" href="https://huggingface.co/Open-Orca/OpenOrca-Platypus2-13B">OpenOrca Platypus2</a><a class="headerlink" href="#openorca-platypus2" title="Permalink to this heading">¶</a></h3>
<p>Conversation template: <code class="docutils literal notranslate"><span class="pre">llama-2</span></code></p>
<table class="docutils align-default" id="id16">
<caption><span class="caption-text">OpenOrca Platypus2</span><a class="headerlink" href="#id16" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Size</p></th>
<th class="head"><p>Hugging Face Repo Link</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>13B</p></td>
<td><p><a class="reference external" href="https://huggingface.co/DavidSharma/mlc-chat-OpenOrca-Platypus2-13B-q4f16_1">q4f16_1</a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="flagalpha-llama-2-chinese">
<span id="flag-alpha-llama2-variant-table"></span><h3><a class="reference external" href="https://github.com/FlagAlpha/Llama2-Chinese">FlagAlpha Llama-2 Chinese</a><a class="headerlink" href="#flagalpha-llama-2-chinese" title="Permalink to this heading">¶</a></h3>
<p>Conversation template: <code class="docutils literal notranslate"><span class="pre">llama-2</span></code></p>
<table class="docutils align-default" id="id17">
<caption><span class="caption-text">FlagAlpha Llama-2 Chinese</span><a class="headerlink" href="#id17" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Size</p></th>
<th class="head"><p>Hugging Face Repo Link</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>7B</p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-FlagAlpha-Llama2-Chinese-7b-Chat-q4f16_1">q4f16_1</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-FlagAlpha-Llama2-Chinese-7b-Chat-q4f32_1">q4f32_1</a></p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</section>
<section id="llama2-uncensored-georgesung">
<span id="llama2-uncensored-variant-table"></span><h3><a class="reference external" href="https://huggingface.co/georgesung/llama2_7b_chat_uncensored">Llama2 uncensored (georgesung)</a><a class="headerlink" href="#llama2-uncensored-georgesung" title="Permalink to this heading">¶</a></h3>
<p>Conversation template: <code class="docutils literal notranslate"><span class="pre">llama-default</span></code></p>
<table class="docutils align-default" id="id18">
<caption><span class="caption-text">Llama2 uncensored</span><a class="headerlink" href="#id18" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Size</p></th>
<th class="head"><p>Hugging Face Repo Link</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>7B</p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-georgesung-llama2-7b-chat-uncensored-q4f16_1">q4f16_1</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-georgesung-llama2-7b-chat-uncensored-q4f32_1">q4f32_1</a></p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</section>
<section id="redpajama">
<span id="red-pajama-variant-table"></span><h3><a class="reference external" href="https://www.together.xyz/blog/redpajama">RedPajama</a><a class="headerlink" href="#redpajama" title="Permalink to this heading">¶</a></h3>
<p>Conversation template: <code class="docutils literal notranslate"><span class="pre">LM</span></code></p>
<table class="docutils align-default" id="id19">
<caption><span class="caption-text">Red Pajama</span><a class="headerlink" href="#id19" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Size</p></th>
<th class="head"><p>Hugging Face Repo Link</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>3B</p></td>
<td><ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/RedPajama-INCITE-Instruct-3B-v1-q4f16_0">q4f16_0 (Instruct)</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f16_0">q4f16_0 (Chat)</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f16_1">q4f16_1 (Chat)</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-RedPajama-INCITE-Chat-3B-v1-q4f32_0">q4f32_0 (Chat)</a></p></li>
</ul>
</td>
</tr>
</tbody>
</table>
</section>
<section id="rwkv-raven">
<span id="rwkv-raven-variant-table"></span><h3><a class="reference external" href="https://github.com/BlinkDL/RWKV-LM">RWKV-raven</a><a class="headerlink" href="#rwkv-raven" title="Permalink to this heading">¶</a></h3>
<p>Conversation template: <code class="docutils literal notranslate"><span class="pre">rwkv</span></code></p>
<table class="docutils align-default" id="id20">
<caption><span class="caption-text">RWKV-raven</span><a class="headerlink" href="#id20" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Size</p></th>
<th class="head"><p>Hugging Face Repo Link</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1B5</p></td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-rwkv-raven-1b5-q8f16_0">q8f16_0</a></p></td>
</tr>
<tr class="row-odd"><td><p>3B</p></td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-rwkv-raven-3b-q8f16_0">q8f16_0</a></p></td>
</tr>
<tr class="row-even"><td><p>7B</p></td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-rwkv-raven-7b-q8f16_0">q8f16_0</a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="wizardcoder">
<span id="wizard-coder-variant-table"></span><h3><a class="reference external" href="https://github.com/nlpxucan/WizardLM">WizardCoder</a><a class="headerlink" href="#wizardcoder" title="Permalink to this heading">¶</a></h3>
<p>Conversation template: <code class="docutils literal notranslate"><span class="pre">wizard_coder_or_math</span></code></p>
<table class="docutils align-default" id="id21">
<caption><span class="caption-text">WizardCoder</span><a class="headerlink" href="#id21" title="Permalink to this table">¶</a></caption>
<colgroup>
<col style="width: 50%" />
<col style="width: 50%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Size</p></th>
<th class="head"><p>Hugging Face Repo Link</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>15B</p></td>
<td><p><a class="reference external" href="https://huggingface.co/mlc-ai/mlc-chat-WizardCoder-15B-V1.0-q4f16_1">q4f16_1</a></p></td>
</tr>
</tbody>
</table>
<hr class="docutils" />
</section>
</section>
<section id="contribute-models-to-mlc-llm">
<span id="id2"></span><h2><a class="toc-backref" href="#id46" role="doc-backlink">Contribute Models to MLC-LLM</a><a class="headerlink" href="#contribute-models-to-mlc-llm" title="Permalink to this heading">¶</a></h2>
<p>Ready to contribute your compiled models/new model architectures? Awesome! Please check <a class="reference internal" href="community/guideline.html#contribute-new-models"><span class="std std-ref">Contribute New Models to MLC-LLM</span></a> on how to contribute new models to MLC-LLM.</p>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="install/tvm.html" class="btn btn-neutral float-right" title="Install TVM Unity Compiler" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="tutorials/customize/define_new_models.html" class="btn btn-neutral float-left" title="Define New Model Architectures" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023 MLC LLM</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote"> </div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>